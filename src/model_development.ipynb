{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "This dataset is designed to predict the selling price of used vehicles. The original dataset contains the following features:\n",
    "\n",
    "- **Car_Name**: Name of the vehicle.  \n",
    "- **Year**: Year of manufacture.  \n",
    "- **Selling_Price**: Final selling price of the vehicle (target variable).  \n",
    "- **Present_Price**: Current market price of the vehicle.  \n",
    "- **Driven_Kms**: Total kilometers driven.  \n",
    "- **Fuel_Type**: Type of fuel used (e.g., Petrol, Diesel, CNG).  \n",
    "- **Selling_Type**: Type of seller (Dealer or Individual).  \n",
    "- **Transmission**: Type of transmission (Manual or Automatic).  \n",
    "- **Owner**: Number of previous owners.  \n",
    "\n",
    "The dataset has already been split into training and testing sets, which are available in the `files/input/` directory.\n",
    "\n",
    "The following sections describe the methodology used to build the predictive model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Decisions\n",
    "\n",
    "### 1. Handling Missing and Duplicate Values\n",
    "\n",
    "Rows containing missing values were removed to ensure data consistency and avoid bias introduced by incomplete records.  \n",
    "Duplicate entries were also eliminated to prevent distortion in model training.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Feature Engineering: Vehicle Age\n",
    "\n",
    "Instead of using the raw `Year` feature, a new variable `Age` was created:\n",
    "\n",
    "Age = 2021 - Year\n",
    "\n",
    "This transformation improves interpretability and captures the real factor influencing depreciation: the vehicle’s age rather than its manufacturing year.\n",
    "\n",
    "The original `Year` column was removed to avoid redundancy.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Removal of `Car_Name`\n",
    "\n",
    "`Car_Name` was excluded because it is a high-cardinality categorical variable that does not directly contribute structured numerical information without advanced encoding techniques.\n",
    "\n",
    "Removing it simplifies the model while maintaining relevant predictive power.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Logarithmic Transformation of Price Variables\n",
    "\n",
    "A logarithmic transformation (`log1p`) was applied to:\n",
    "\n",
    "- `Present_Price`\n",
    "- `Selling_Price`\n",
    "\n",
    "#### Why?\n",
    "\n",
    "Price-related variables are typically right-skewed, meaning:\n",
    "\n",
    "- A few high-value vehicles distort the distribution\n",
    "- Variance increases with magnitude\n",
    "\n",
    "Applying a log transformation:\n",
    "\n",
    "- Stabilizes variance\n",
    "- Reduces skewness\n",
    "- Improves linear model performance\n",
    "- Makes relationships more linear\n",
    "\n",
    "The use of `log1p` (log(1 + x)) ensures numerical stability when values are close to zero.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Proper Train/Test Separation\n",
    "\n",
    "The dataset was split into:\n",
    "\n",
    "- Feature matrix (`X`)\n",
    "- Target variable (`y`)\n",
    "\n",
    "This structure ensures:\n",
    "\n",
    "- Clear separation between predictors and outcome\n",
    "- Clean pipeline integration\n",
    "- Reliable evaluation on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import json\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,f_regression, mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def carga_y_limpieza(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Loads training and testing datasets, performs cleaning,\n",
    "    feature engineering, and basic transformations.\n",
    "\n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        categorical_columns, numerical_columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    print(\"Train shape:\", train_df.shape)\n",
    "    print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Data Cleaning\n",
    "    # ---------------------------\n",
    "\n",
    "    # Remove missing values\n",
    "    train_df.dropna(inplace=True)\n",
    "    test_df.dropna(inplace=True)\n",
    "\n",
    "    # Remove duplicate records (if any)\n",
    "    train_df.drop_duplicates(inplace=True)\n",
    "    test_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Feature Engineering\n",
    "    # ---------------------------\n",
    "\n",
    "    # Create vehicle age assuming reference year = 2021\n",
    "    train_df[\"Age\"] = 2021 - train_df[\"Year\"]\n",
    "    test_df[\"Age\"] = 2021 - test_df[\"Year\"]\n",
    "\n",
    "    # Drop non-informative or redundant columns\n",
    "    train_df.drop([\"Car_Name\", \"Year\"], axis=1, inplace=True)\n",
    "    test_df.drop([\"Car_Name\", \"Year\"], axis=1, inplace=True)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Log Transformation\n",
    "    # ---------------------------\n",
    "\n",
    "    # Apply log1p transformation to stabilize variance\n",
    "    # and reduce skewness in price-related features\n",
    "    price_columns = [\"Present_Price\", \"Selling_Price\"]\n",
    "\n",
    "    for col in price_columns:\n",
    "        train_df[col] = np.log1p(train_df[col])\n",
    "        test_df[col] = np.log1p(test_df[col])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Feature / Target Separation\n",
    "    # ---------------------------\n",
    "\n",
    "    # Target variable: Present_Price\n",
    "    y_train = train_df[\"Present_Price\"]\n",
    "    X_train = train_df.drop(columns=[\"Present_Price\"])\n",
    "\n",
    "    y_test = test_df[\"Present_Price\"]\n",
    "    X_test = test_df.drop(columns=[\"Present_Price\"])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Column Classification\n",
    "    # ---------------------------\n",
    "\n",
    "    categorical_columns = [\"Fuel_Type\", \"Selling_type\", \"Transmission\"]\n",
    "    numerical_columns = [\"Selling_Price\", \"Driven_kms\", \"Owner\", \"Age\"]\n",
    "\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, categorical_columns, numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Pipeline\n",
    "\n",
    "A machine learning pipeline was constructed to ensure a structured, reproducible, and scalable modeling workflow. The pipeline integrates preprocessing, feature selection, and model training into a single unified framework.\n",
    "\n",
    "### 1. Categorical Encoding\n",
    "\n",
    "Categorical variables were transformed using **One-Hot Encoding**.\n",
    "\n",
    "This approach:\n",
    "- Converts categorical features into a numerical representation\n",
    "- Prevents unintended ordinal relationships\n",
    "- Allows linear models to interpret categorical information correctly\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Feature Scaling\n",
    "\n",
    "Numerical variables were scaled to the range **[0, 1]** using Min-Max Scaling.\n",
    "\n",
    "Scaling was applied to:\n",
    "\n",
    "- Ensure numerical stability\n",
    "- Prevent features with larger magnitudes from dominating the model\n",
    "- Improve convergence behavior in linear regression\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Feature Selection\n",
    "\n",
    "A **K-best feature selection** method was applied to retain the most informative predictors.\n",
    "\n",
    "This step:\n",
    "- Reduces dimensionality\n",
    "- Minimizes noise\n",
    "- Helps prevent overfitting\n",
    "- Improves interpretability\n",
    "\n",
    "Feature selection was performed using statistical relevance scoring.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Model Training\n",
    "\n",
    "A **Linear Regression** model was used as the final estimator.\n",
    "\n",
    "Linear regression was selected because:\n",
    "\n",
    "- It provides interpretability\n",
    "- It establishes a strong baseline model\n",
    "- It performs well when relationships are approximately linear\n",
    "- It allows direct evaluation of feature influence\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use a Pipeline?\n",
    "\n",
    "The pipeline structure ensures:\n",
    "\n",
    "- Clean separation of preprocessing and modeling logic\n",
    "- Prevention of data leakage\n",
    "- Reproducibility\n",
    "- Easier deployment and scalability\n",
    "- Compatibility with cross-validation and hyperparameter tuning\n",
    "\n",
    "By integrating all steps into a single pipeline, the workflow becomes robust, modular, and production-oriented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "\n",
    "def build_pipeline(estimator, categorical_columns=None, numeric_columns=None):\n",
    "    \"\"\"\n",
    "    Builds a complete machine learning pipeline including:\n",
    "    - Categorical encoding\n",
    "    - Numerical scaling\n",
    "    - Feature selection\n",
    "    - Model training\n",
    "\n",
    "    Parameters:\n",
    "        estimator: sklearn-compatible model\n",
    "        categorical_columns: list of categorical feature names\n",
    "        numeric_columns: list of numerical feature names\n",
    "\n",
    "    Returns:\n",
    "        sklearn Pipeline object\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------\n",
    "    # Preprocessing Stage\n",
    "    # ---------------------------\n",
    "\n",
    "    # Apply One-Hot Encoding to categorical features\n",
    "    # Apply Min-Max Scaling to numerical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"categorical\",\n",
    "             OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "             categorical_columns),\n",
    "\n",
    "            (\"numerical\",\n",
    "             MinMaxScaler(),\n",
    "             numeric_columns)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # Full Pipeline\n",
    "    # ---------------------------\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "\n",
    "        # Step 1: Feature preprocessing\n",
    "        (\"preprocessor\", preprocessor),\n",
    "\n",
    "        # Step 2: Feature selection (top K best features)\n",
    "        # Score function can be specified depending on regression/classification task\n",
    "        (\"feature_selection\", SelectKBest()),\n",
    "\n",
    "        # Step 3: Final estimator (e.g., LinearRegression)\n",
    "        (\"model\", estimator)\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "Model hyperparameters were optimized using **10-fold cross-validation** to ensure robust performance estimation and reduce overfitting.\n",
    "\n",
    "In each iteration:\n",
    "- The model was trained on 9 folds\n",
    "- Validated on the remaining fold\n",
    "- Final performance was averaged across all splits\n",
    "\n",
    "### Evaluation Metric\n",
    "\n",
    "Performance was measured using **Mean Absolute Error (MAE)**.\n",
    "\n",
    "MAE was selected because:\n",
    "- It provides interpretable error values in the same unit as the target variable\n",
    "- It is less sensitive to outliers than squared-error metrics\n",
    "- It reflects real-world deviation in price prediction\n",
    "\n",
    "This approach ensures reliable model selection and strong generalization to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid_search(estimator, param_grid, cv=10):\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "    )\n",
    "\n",
    "    return grid_search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence\n",
    "\n",
    "Once the final model was trained and validated, it was serialized and stored for future use.\n",
    "\n",
    "The model was saved in compressed format using `gzip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_estimator(estimator):\n",
    "\n",
    "    os.makedirs(\"../files/models\", exist_ok=True)\n",
    "    with gzip.open(\"../files/models/model.pkl.gz\", \"wb\") as f:\n",
    "        pickle.dump(estimator, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Metrics Export\n",
    "\n",
    "The final model was evaluated on both training and testing datasets using multiple regression metrics to assess performance and generalization.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "The following metrics were computed:\n",
    "\n",
    "- **R² (Coefficient of Determination)** – Measures the proportion of variance explained by the model.\n",
    "- **Mean Squared Error (MSE)** – Penalizes larger errors more heavily.\n",
    "- **Mean Absolute Error (MAE)** – Measures average absolute prediction error.\n",
    "\n",
    "Evaluating both training and test sets allows detection of potential overfitting or underfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics Persistence\n",
    "\n",
    "All evaluation results were exported to: files/output/metrics.json\n",
    "\n",
    "Each line in the file contains a dictionary with the following structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"type\": \"metrics\",\n",
    "    \"dataset\": \"train\" or \"test\",\n",
    "    \"r2\": float,\n",
    "    \"mse\": float,\n",
    "    \"mae\": float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_metricas(best_model,y_train, y_train_pred, y_test, y_test_pred, output_path):\n",
    "    train_metrics = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'train',\n",
    "        'r2': r2_score(y_train, y_train_pred),\n",
    "        'mse': mean_squared_error(y_train, y_train_pred),\n",
    "        'mad': mean_absolute_error(y_train, y_train_pred)\n",
    "    }\n",
    "\n",
    "    \n",
    "    test_metrics = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'test',\n",
    "        'r2': r2_score(y_test, y_test_pred),\n",
    "        'mse': mean_squared_error(y_test, y_test_pred),\n",
    "        'mad': mean_absolute_error(y_test, y_test_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(json.dumps(train_metrics) + '\\n')\n",
    "        f.write(json.dumps(test_metrics) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (211, 9)\n",
      "Test shape: (90, 9)\n",
      "X_train shape: (210, 7)\n",
      "X_test shape: (90, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('categorical',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['Fuel_Type', 'Selling_type',\n",
      "                                                   'Transmission']),\n",
      "                                                 ('numerical', MinMaxScaler(),\n",
      "                                                  ['Selling_Price',\n",
      "                                                   'Driven_kms', 'Owner',\n",
      "                                                   'Age'])])),\n",
      "                ('feature_selection',\n",
      "                 SelectKBest(score_func=<function f_regression at 0x115921800>)),\n",
      "                ('model', LinearRegression())])\n"
     ]
    }
   ],
   "source": [
    "def linear_regression():\n",
    "    train_path=\"../data/raw/train_data.csv\"\n",
    "    test_path=\"../data/raw/test_data.csv\"\n",
    "\n",
    "   \n",
    "    x_train, y_train, x_test, y_test, categorical_columns, numeric_columns = carga_y_limpieza(train_path, test_path)\n",
    "\n",
    "    \n",
    "\n",
    "       \n",
    "    pipeline = build_pipeline(\n",
    "        estimator=LinearRegression(),\n",
    "        categorical_columns=categorical_columns,\n",
    "        numeric_columns=numeric_columns,\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'feature_selection__k': [10],\n",
    "        'feature_selection__score_func': [f_regression], \n",
    "    }\n",
    "        \n",
    "    grid_search = make_grid_search(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=10,\n",
    "    )\n",
    "\n",
    "    grid_search.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    save_estimator(grid_search)\n",
    "    y_train_pred = best_model.predict(x_train)\n",
    "    y_test_pred = best_model.predict(x_test)\n",
    "    \n",
    "    print(best_model)\n",
    "\n",
    "\n",
    "    output_path = \"../outputs/metrics.json\"\n",
    "    guardar_metricas(best_model, y_train, y_train_pred, y_test, y_test_pred, output_path)\n",
    "\n",
    "linear_regression()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
